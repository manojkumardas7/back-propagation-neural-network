{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:block\">\n",
    "    <div style=\"width: 20%; display: inline-block; text-align: left;\">\n",
    "    </div>\n",
    "    <div style=\"width: 59%; display: inline-block\">\n",
    "        <h1  style=\"text-align: center\">Back Propagation Neural Network</h1>\n",
    "        <div style=\"width: 85%; text-align: center; display: inline-block;\"><i>Authors:</i> <strong>Manoj Kumar Das </strong> </div>\n",
    "    </div>\n",
    "    <div style=\"width: 20%; text-align: right; display: inline-block;\">\n",
    "        <div style=\"width: 100%; text-align: left; display: inline-block;\">\n",
    "            <i>Created: </i>\n",
    "            <time datetime=\"2014-03-10\" pubdate>March 10, 2016</time>\n",
    "        </div>\n",
    "        <div style=\"width: 100%; text-align: left; display: inline-block;\">\n",
    "            <i>Modified: </i>\n",
    "            <time datetime=\"2014-03-12\" pubdate>March 10, 2016</time>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment, we attempt to write a python code to replicate a back propagation neural network and have it trained to perform as a classifier over the MNIST data (images of digits from 0 to 9). The objective of this assignment is to have a highly flexible, generic and functional(yeah!!!) code so as to experiment the performance of the network having the possibility to change any of the architectural parameters like the number of hidden layers and the number of neurons in them, activation function of choice in each layer. Basic equations used to write the code are presented, without a complete an in-depth explanation the functioning of back propagation neural network and the gradient descent algorithm.\n",
    "We have 60,000 samples of training data containing images of digits from 0 to 9. The neural network will be trained over all the 60,000 samples for a fixed number of epochs and again will be made to predict over 10,000 samples of a validation set to set to which class they belong to.\n",
    "\n",
    "## Details of Neural Network:\n",
    "In this assignment, we have chosen to test a network with only one hidden layer. Obviously, the number of neurons in the input layer is equal to the number of pixels we have coming from the image, and 10 output neurons, coming from the output layer corresponding to the 10 classes. In the training set the desired output of each neuron will be set to zeros except for the class the image belongs to which will set to one. On validation of the network's prediction, we consider the class corresponding to the neuron which gives out the maximum value.\n",
    "The number of neurons in the hidden layer is chosen to be 149, this is obtained by taking the average(closest integer) number of non-zero pixels over all the 60,000 samples of the training dataset.\n",
    "Equations for the forward pass and reverse pass are as below:\n",
    "\n",
    "$z_{in_{j}}~=~\\sum{x_{i}  \\textit{v}_{i,j}}$\n",
    "\n",
    "$z_{j}~=~f(z_{in_{j}})$,  activation function of the hidden layer\n",
    "\n",
    "$y_{in_{k}}~=~\\sum{z_{j}  w_{j,k}}$\n",
    "\n",
    "$\\Delta{w_{j,k}}~=~\\alpha\\delta_{k}\\textit{z}_{j}$ where $\\delta_{k}~=~(y_{dk}-y_{k})~f^{'}(y_{in_{k}})$\n",
    "\n",
    "$\\Delta{v_{i,j}}~=~\\alpha\\delta_{j}x_{j}$ where $\\delta_{j}~=~\\delta_{in_{j}}~f^{'}(z_{in_{j}})$ and $\\delta_{in_{j}}~=~\\sum{\\delta_{k}w_{j,k}}$\n",
    "\n",
    "$E~=~\\frac{1}{2}\\sum{(y_{dk}-y_{k})^{2}}$, sum square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\">\n",
    "<input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import struct\n",
    "import msgpack\n",
    "import cPickle\n",
    "import bigfloat \n",
    "import numpy as np\n",
    "from array import array\n",
    "# from cvxopt.base import matrix\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "trainImages are the images used for training and train Labels are their corresponding labels. Similarly testImages and testLables are what will be used for the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsFile = open('train-labels.idx1-ubyte', 'rb')\n",
    "magic_nr, size = struct.unpack(\">II\", labelsFile.read(8))\n",
    "trainLabels = map(lambda x: np.eye(10)[x].reshape(1,-1), array(\"b\", labelsFile.read())) \n",
    "labelsFile.close()\n",
    "\n",
    "imagesFile = open('train-images.idx3-ubyte', 'rb')\n",
    "magic_nr, size, rows, cols = struct.unpack(\">IIII\", imagesFile.read(16))\n",
    "trainImages = np.vectorize(lambda x: float(x *1.0 / 255 ))(array(\"B\", imagesFile.read())).reshape(-1, rows * cols)\n",
    "trainImages = np.array(map(lambda x: x.reshape(1,-1), trainImages))\n",
    "trainLabels = np.array(map(lambda x: np.array(x).reshape(1,-1), trainLabels))\n",
    "imagesFile.close()\n",
    "\n",
    "labelsFile = open('t10k-labels.idx1-ubyte', 'rb')\n",
    "magic_nr, size = struct.unpack(\">II\", labelsFile.read(8))\n",
    "testLabels = map(lambda x: np.eye(10)[x].reshape(1,-1), array(\"b\", labelsFile.read())) \n",
    "labelsFile.close()\n",
    "\n",
    "imagesFile = open('t10k-images.idx3-ubyte', 'rb')\n",
    "magic_nr, size, rows, cols = struct.unpack(\">IIII\", imagesFile.read(16))\n",
    "testImages = np.vectorize(lambda x: float(x *1.0 / 255 ))(array(\"B\", imagesFile.read())).reshape(-1, rows * cols)\n",
    "testImages = np.array(map(lambda x: x.reshape(1,-1), testImages))\n",
    "testLabels = np.array(map(lambda x: np.array(x).reshape(1,-1), testLabels))\n",
    "imagesFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code\n",
    "\n",
    "Below is the main code block for the back propagation, the redMap function does the forward pass, and backRedMap function performs the reverse pass which updates the weights. The activation functions and their corresponding derivatives are defined which will be used by the two mentioned functions respectively. The function fronAndBackPass runs over the former two functions for a given sample and evaluates the loss value for the new updated weights. The function gradientDescent repeats frontAndBackPass every sample.The final line in below code block repeats the gradientDescent function according to the number of epochs is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mySoftmax(values):\n",
    "    e = np.vectorize(lambda x: bigfloat.exp(x))(values)\n",
    "    return np.vectorize(lambda x: float(x))(e / np.sum(e))\n",
    "    \n",
    "def redMap(redMapSeqFunc, redMapSeq):\n",
    "    redmapResult = [redMapSeqFunc[0](redMapSeq[0], redMapSeq[1])]\n",
    "    map(lambda x, y: redmapResult.append(x(redmapResult[-1], y)), redMapSeqFunc[1:], redMapSeq[2:]);\n",
    "    return redmapResult\n",
    "\n",
    "def backRedMap(backRedMapSeqFunc, backRedMapSeq, inPut, tarGet, w, alpha):\n",
    "    ro = [(tarGet-backRedMapSeq[-1]) * backRedMapSeqFunc[0](backRedMapSeq[-1])]\n",
    "    map(lambda x, y, z: ro.insert(0, np.dot(ro[0], x.T) * y(z)), reversed(w[1:]), backRedMapSeqFunc[1:],\n",
    "        reversed(backRedMapSeq[:-1]));\n",
    "    backRedMapResult = map(lambda x, y: alpha * np.dot(x.T,y), [inPut] + backRedMapSeq[:-1], ro)\n",
    "    return backRedMapResult\n",
    "    \n",
    "def frontAndBackPass(inPut, tarGet):\n",
    "    global w, actFuncs, actDeriveFuncs, sumSquareError, alpha, regLambdaDiff    \n",
    "    w = map(lambda x,y: (regLambdaDiff * x) + y, w, \n",
    "            backRedMap(actDeriveFuncs,\n",
    "            redMap(actFuncs, [inPut] + w), inPut, tarGet, w, alpha));\n",
    "    sumSquareError.extend([np.sum((tarGet - redMap(actFuncs, [inPut] + w)[-1]) ** 2) * 1.0 / 2])\n",
    "\n",
    "def gradientDescent(epoch):\n",
    "    global inSamples, outSamples\n",
    "    map(lambda x, y: frontAndBackPass(x, y), inSamples, outSamples)\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(\"[%s] %s%d\" % ('='*(epoch + 1), 'epochs: ', epoch + 1))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "sigmoid = lambda x, y: expit(np.dot(x, y))\n",
    "tanh = lambda x, y: np.tanh(np.dot(x, y))\n",
    "relu = lambda x, y: np.maximum(0,np.dot(x, y))\n",
    "softmax = lambda x, y: mySoftmax(np.dot(x, y))\n",
    "\n",
    "sigmoidDerive = np.vectorize(lambda x: x * (1-x))\n",
    "tanhDerive = np.vectorize(lambda x: 1 - (x ** 2)) \n",
    "reluDerive = np.vectorize(lambda x: int(x > 0))\n",
    "softmaxDerive = np.vectorize(lambda x: x * (1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Demonstration\n",
    "\n",
    "Now we are going to just perform a training with 1000 epochs for only the first 10 samples and view how the gradient descent algorithm updates the weights of the network as we back propagate across the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10 #Chossing only first 10 samples\n",
    "'''Setting the number of neuron of the hidden layer to the average number of non-zero pixels across all samples'''\n",
    "hiddenSpecs = np.sum(map(lambda x: len(x[x>0]), trainImages))/trainImages.shape[0]  \n",
    "actSpecs = ['relu', 'softmax'] #Chosen activation functions\n",
    "alpha = 1e-01\n",
    "regLambda = 0 #Not going for any regulation\n",
    "epochs = 1000\n",
    "\n",
    "inSamples = trainImages[:samples]\n",
    "outSamples = trainLabels[:samples]\n",
    "sumSquareError = []\n",
    "regLambdaDiff = 1 - regLambda\n",
    "actFuncs = eval(str(actSpecs).translate(None,\"'\"))\n",
    "actDeriveFuncs = eval(str(map(lambda x: x + 'Derive', reversed(actSpecs))).translate(None,\"'\"))\n",
    "\n",
    "'''Create a list for number of nodes for every hidden layer. The number of values in the list \n",
    "determines the number of hidden layers and the value itself correpsonds to the number of nodes respectively \n",
    "for each hidden layer. Specifying the needed activation function for every layer''' \n",
    "netArch = np.hstack((inSamples.shape[2],hiddenSpecs, outSamples.shape[2]))\n",
    "\n",
    "#Random intial weight generation\n",
    "w = map(lambda x, y: 0.001 * np.random.rand(x, y), netArch[:-1], netArch[1:])\n",
    "\n",
    "#Training execution\n",
    "map(lambda x: gradientDescent(x), np.arange(epochs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sumSquareError)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('sum square error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in xrange(10) :\n",
    "    print np.argmax(redMap(actFuncs, [inSamples[index]] + w)[-1]),\n",
    "    print np.argmax(outSamples[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full experiment results\n",
    "\n",
    "We have trained the above network for 500 epochs over all the 60,000 samples. No regulation was set and the learning rate,  αα  was set to 0.01. The weights are then saved, which we shall now utilize and test and see how the network performs over the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing  the weights of the trained network\n",
    "w = cPickle.load(open('weightFile.p', 'rb')) \n",
    "\n",
    "'''Genrating compare array which conatins two colums, the first column is the predicted class for every sample and\n",
    "the second column the actual class the sample belongs to'''\n",
    "compare = np.array(map(lambda index: [np.argmax(redMap(actFuncs, [testImages[index]] + w)[-1]),\n",
    "                                  np.argmax(testLabels[index])], np.arange(testLabels.shape[0])))\n",
    "\n",
    "'''Computing the accuracy, this is done by subtracting the two columns of the above array, if the prediction is \n",
    "incorrect you get a non-zero value, thereford by couting the number non-zero values you get the number of wrong\n",
    "predictions'''\n",
    "accuracy = (testLabels.shape[0] - np.nonzero(compare[:,0]-compare[:,1])[0].shape[0]) * 100.0 / testLabels.shape[0]\n",
    "\n",
    "print 'number of incorrect predictions = ' + str(np.nonzero(compare[:,0] - compare[:,1])[0].shape[0])\n",
    "print 'accuracy = ' + str(accuracy) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "There can be improvements done on this experiment by varying the learning rate and going for regulation. This code also enables us to go for a second layer or as many layers desired. The leaky Relu and markout activation function are yet to be implemented. Moreover, softmax activation can be replaced with sigmoid to get a similar result and decrease computation time. This assignment hopefully shows how the neural network less or more performs but there is much that can be done and should be experimented to improve the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
